{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders_cifar10_selfsupervised(batch_size, num_workers=0,\n",
    "                            validation_fraction=None,\n",
    "                            train_transforms=None,\n",
    "                            test_transforms=None,\n",
    "                            mode = 'supervised'):\n",
    "\n",
    "    if train_transforms is None:\n",
    "        train_transforms = transforms.ToTensor()\n",
    "    if test_transforms is None:\n",
    "        test_transforms = transforms.ToTensor()\n",
    "\n",
    "    train_dataset = datasets.CIFAR10(root='data',\n",
    "                                    train=True,\n",
    "                                    transform=train_transforms,\n",
    "                                    download=True)\n",
    "    valid_dataset = datasets.CIFAR10(root='data',\n",
    "                                    train=True,\n",
    "                                    transform=test_transforms)\n",
    "    test_dataset = datasets.CIFAR10(root='data',\n",
    "                                    train=False,\n",
    "                                    transform=test_transforms)\n",
    "\n",
    "    if mode == 'supervised':\n",
    "        num = int(validation_fraction * 50000)\n",
    "        train_indices = torch.arange(0, 50000 - num)\n",
    "        valid_indices = torch.arange(50000 - num, 50000)\n",
    "        train_sampler = SubsetRandomSampler(train_indices)\n",
    "        valid_sampler = SubsetRandomSampler(valid_indices)\n",
    "        valid_loader = DataLoader(dataset=valid_dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                num_workers=num_workers,\n",
    "                                sampler=valid_sampler)\n",
    "        train_loader = DataLoader(dataset=train_dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                num_workers=num_workers,\n",
    "                                drop_last=True,\n",
    "                                sampler=train_sampler)\n",
    "        test_loader = DataLoader(dataset=test_dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                num_workers=num_workers,\n",
    "                                shuffle=False)\n",
    "        return train_loader, valid_loader, test_loader\n",
    "\n",
    "    elif mode == 'self-supervised':\n",
    "        num = int(validation_fraction * 50000)\n",
    "        valid_indices = torch.arange(50000 - num, 50000)\n",
    "        train_indices = torch.arange(0, 50000 - num)\n",
    "        \n",
    "        train_dataset = torch.utils.data.Subset(train_dataset, train_indices)\n",
    "        valid_dataset = torch.utils.data.Subset(valid_dataset, valid_indices)\n",
    "\n",
    "        train_unlabelled_indices, train_labelled_indices = train_test_split(list(range(len(train_dataset))), test_size=5000)\n",
    "        train_unlabelled_dataset = torch.utils.data.Subset(train_dataset, train_unlabelled_indices)\n",
    "        train_labelled_dataset = torch.utils.data.Subset(train_dataset, train_labelled_indices)\n",
    "        valid_loader = DataLoader(dataset=valid_dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                num_workers=num_workers)\n",
    "\n",
    "        train_labelled_loader = DataLoader(dataset=train_labelled_dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                num_workers=num_workers,\n",
    "                                drop_last=True)\n",
    "        train_unlabelled_loader = DataLoader(dataset=train_unlabelled_dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                num_workers=num_workers,\n",
    "                                drop_last=True)\n",
    "        test_loader = DataLoader(dataset=test_dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                num_workers=num_workers,\n",
    "                                shuffle=False)\n",
    "\n",
    "        return train_labelled_loader, train_unlabelled_loader, valid_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# For Supervised\n",
    "train_transforms, test_transforms = None, None\n",
    "train_labelled_loader, train_unlabelled_loader, valid_loader, test_loader = get_dataloaders_cifar10_selfsupervised(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_fraction=0.1,\n",
    "    train_transforms=train_transforms,\n",
    "    test_transforms=test_transforms,\n",
    "    num_workers=2, mode = 'self-supervised')\n",
    "\n",
    "train_transforms = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: torch.flatten(x))])\n",
    "test_transforms = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: torch.flatten(x))])\n",
    "train_labelled_loader_flatten, train_unlabelled_loader_flatten, valid_loader_flatten, test_loader_flatten = get_dataloaders_cifar10_selfsupervised(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_fraction=0.1,\n",
    "    train_transforms=train_transforms,\n",
    "    test_transforms=test_transforms,\n",
    "    num_workers=2, mode = 'self-supervised')\n",
    "\n",
    "for images, labels in train_labelled_loader:  \n",
    "    print('Image batch dimensions:', images.shape)\n",
    "    print('Image label dimensions:', labels.shape)\n",
    "    print('Class labels of 10 examples:', labels[:10])\n",
    "    print()\n",
    "    break\n",
    "\n",
    "for images, labels in train_labelled_loader_flatten:  \n",
    "    print('Image batch dimensions:', images.shape)\n",
    "    print('Image label dimensions:', labels.shape)\n",
    "    print('Class labels of 10 examples:', labels[:10])\n",
    "    print()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_selfsupervised(model, num_epochs, train_loader_lst,\n",
    "                valid_loader, test_loader, optimizer,\n",
    "                device, logging_interval=50,\n",
    "                scheduler=None,\n",
    "                scheduler_on='valid_acc'):\n",
    "    train_loader, train_unlabelled_loader = train_loader_lst\n",
    "    start_time = time.time()\n",
    "    minibatch_loss_list, train_acc_list, valid_acc_list = [], [], []\n",
    "    cnt = 0\n",
    "    predicted_labels_dict = {}\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        model.train()\n",
    "        \n",
    "        if cnt > 0:\n",
    "            for batch_idx, (features, targets) in itertools.islice(enumerate(train_unlabelled_loader), 0, (cnt)*BATCH_SIZE, 1):\n",
    "                features = features.to(device)\n",
    "                targets = targets.float().to(device)\n",
    "\n",
    "                logits = model(features)\n",
    "                _, predicted_labels = torch.max(logits, 1)\n",
    "\n",
    "                targets = predicted_labels_dict[batch_idx].to(device)\n",
    "\n",
    "                # ## FORWARD AND BACK PROP\n",
    "                logits = model(features)\n",
    "                loss = torch.nn.functional.cross_entropy(logits, targets)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                # ## UPDATE MODEL PARAMETERS\n",
    "                optimizer.step()\n",
    "\n",
    "                # ## LOGGING\n",
    "                minibatch_loss_list.append(loss.item())\n",
    "                if not batch_idx % logging_interval:\n",
    "                    print(f'Self-Supervised '\n",
    "                        f'| Batch {batch_idx:04d}/{len(train_unlabelled_loader):04d} '\n",
    "                        f'| Loss: {loss:.4f}')\n",
    "            predicted_labels_dict = {}\n",
    "\n",
    "        for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "\n",
    "            features = features.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # ## FORWARD AND BACK PROP\n",
    "            logits = model(features)\n",
    "            loss = torch.nn.functional.cross_entropy(logits, targets)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # ## UPDATE MODEL PARAMETERS\n",
    "            optimizer.step()\n",
    "\n",
    "            # ## LOGGING\n",
    "            minibatch_loss_list.append(loss.item())\n",
    "            if not batch_idx % logging_interval:\n",
    "                print(f'Epoch: {epoch+1:03d}/{num_epochs:03d} '\n",
    "                      f'| Batch {batch_idx:04d}/{len(train_loader):04d} '\n",
    "                      f'| Loss: {loss:.4f}')\n",
    "\n",
    "        for batch_idx, (features, targets) in itertools.islice(enumerate(train_unlabelled_loader), 0, (cnt+1)*BATCH_SIZE, 1):\n",
    "            features = features.to(device)\n",
    "            targets = targets.float().to(device)\n",
    "\n",
    "            logits = model(features)\n",
    "            _, predicted_labels = torch.max(logits, 1)\n",
    "\n",
    "            targets = predicted_labels.to(device)\n",
    "            predicted_labels_dict[batch_idx] = targets\n",
    "\n",
    "        cnt+=1\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():  # save memory during inference\n",
    "            train_acc = compute_accuracy(model, train_loader, device=device)\n",
    "            valid_acc = compute_accuracy(model, valid_loader, device=device)\n",
    "            print(f'Epoch: {epoch+1:03d}/{num_epochs:03d} '\n",
    "                  f'| Train: {train_acc :.2f}% '\n",
    "                  f'| Validation: {valid_acc :.2f}%')\n",
    "            train_acc_list.append(train_acc.item())\n",
    "            valid_acc_list.append(valid_acc.item())\n",
    "\n",
    "        elapsed = (time.time() - start_time)/60\n",
    "        print(f'Time elapsed: {elapsed:.2f} min')\n",
    "        \n",
    "        if scheduler is not None:\n",
    "\n",
    "            if scheduler_on == 'valid_acc':\n",
    "                scheduler.step(valid_acc_list[-1])\n",
    "            elif scheduler_on == 'minibatch_loss':\n",
    "                scheduler.step(minibatch_loss_list[-1])\n",
    "            else:\n",
    "                raise ValueError(f'Invalid `scheduler_on` choice.')\n",
    "        \n",
    "\n",
    "    elapsed = (time.time() - start_time)/60\n",
    "    print(f'Total Training Time: {elapsed:.2f} min')\n",
    "\n",
    "    test_acc = compute_accuracy(model, test_loader, device=device)\n",
    "    print(f'Test accuracy {test_acc :.2f}%')\n",
    "\n",
    "    return minibatch_loss_list, train_acc_list, valid_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_train_selfsupervised(model, device, NUM_EPOCHS, data_loader, lr = 0.001, FileName = 'model'):\n",
    "    train_labelled_loader, train_unlabelled_loader, valid_loader, test_loader = data_loader\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), momentum=0.9, lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                        factor=0.1,\n",
    "                                                        mode='max',\n",
    "                                                        verbose=True)\n",
    "\n",
    "    minibatch_loss_list, train_acc_list, valid_acc_list = train_model_selfsupervised(\n",
    "        model=model,\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        train_loader_lst=[train_labelled_loader, train_unlabelled_loader],\n",
    "        valid_loader=valid_loader,\n",
    "        test_loader=test_loader,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        scheduler=scheduler,\n",
    "        scheduler_on='valid_acc',\n",
    "        logging_interval=100)\n",
    "\n",
    "    plot_training_loss(minibatch_loss_list=minibatch_loss_list,\n",
    "                    num_epochs=NUM_EPOCHS,\n",
    "                    iter_per_epoch=len(train_labelled_loader),\n",
    "                    results_dir=None,\n",
    "                    averaging_iterations=200)\n",
    "    plt.show()\n",
    "\n",
    "    plot_accuracy(train_acc_list=train_acc_list,\n",
    "                valid_acc_list=valid_acc_list,\n",
    "                results_dir=None)\n",
    "    # plt.ylim([60, 100])\n",
    "    plt.show()\n",
    "\n",
    "    class_dict = {0: 'airplane',\n",
    "              1: 'automobile',\n",
    "              2: 'bird',\n",
    "              3: 'cat',\n",
    "              4: 'deer',\n",
    "              5: 'dog',\n",
    "              6: 'frog',\n",
    "              7: 'horse',\n",
    "              8: 'ship',\n",
    "              9: 'truck'}\n",
    "    mat = compute_confusion_matrix(model=model, data_loader=test_loader, device=device)\n",
    "    plot_confusion_matrix(mat, class_names=class_dict.values())\n",
    "    plt.show()\n",
    "\n",
    "    torch.save(model.state_dict(), f'{FileName}_selfsupervised.ckpt')\n",
    "    return compute_accuracy(model, test_loader, device=device).to('cpu').numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg = VGG16(num_classes=10)\n",
    "model_resnet = ResNet(ResidualBlock, [2, 2, 2]).to(device)\n",
    "model_mlp = MultilayerPerceptron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc_lst = {'vgg':[],'resnet':[],'mlp':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10\n",
    "for model_name, model in zip(['vgg', 'resnet', 'mlp'],[model_vgg, model_resnet, model_mlp]):\n",
    "    for lr in [0.0001,0.0003,0.001,0.003,0.01]:\n",
    "        print(\"*************************************************************************************\")\n",
    "        print(f\"Model: {model_name}, lr: {lr}\")\n",
    "        print(\"*************************************************************************************\")\n",
    "        if model_name == 'mlp':\n",
    "            data_loader = [train_labelled_loader_flatten, train_unlabelled_loader_flatten, valid_loader_flatten, test_loader_flatten]\n",
    "        else:\n",
    "            data_loader = [train_labelled_loader, train_unlabelled_loader, valid_loader, test_loader]\n",
    "        val = start_train_selfsupervised(\n",
    "            model, device, NUM_EPOCHS, data_loader, lr = lr, \n",
    "            FileName = f\"model_{model_name}_{str(lr).split('.')[-1]}\")\n",
    "        test_acc_lst[model_name].append(val)\n",
    "        print(\"*************************************************************************************\")\n",
    "        print(\"*************************************************************************************\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 1, sharex = True)\n",
    "ax[0].plot(test_acc_lst['vgg'])\n",
    "ax[1].plot(test_acc_lst['resnet'])\n",
    "ax[2].plot(test_acc_lst['mlp'])\n",
    "ax[0].set_xticks(np.arange(5))\n",
    "ax[0].set_xticklabels([0.0001,0.0003,0.001,0.003,0.01])\n",
    "ax[0].set_xlabel('Learning Rate')\n",
    "ax[0].set_ylabel('VGG')\n",
    "ax[1].set_ylabel('ResNet')\n",
    "ax[2].set_ylabel('N-layer NN')\n",
    "ax[0].set_title('Test Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6641ef502f503b3f8daf748aef843fb4663d54d152a495a043119a5e022b300f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('Sarth')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
